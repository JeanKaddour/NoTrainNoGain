# Basic hyperparameter for normal BERT pretraining
# working hard here to separate "impl" implementation details and "train" abstract hyperparameters

name: bert-o3

defaults:
  - optim: adam
  - optim_mod: disabled

#optim:
#  lr: 1e-3
#  eps: 1e-12
#  weight_decay: 0.01

limited_decay_keys: [bias, LayerNorm.bias, LayerNorm.weight] # no weight decay for these layers

# data
validation_set:
    enabled: True
    fraction: 0.005
    seed: 0
    il_model: False
    truncate_to: 10000

# Set to integer value to truncate the dataset to this many sequences.
truncate_train_dataset:

# steps:
warmup_steps: 0
cooldown_steps: 0
steps: 2000000 # these are microbatch steps
scheduler: budget-one-cycle-seconds

# Training settting:
batch_size: 1536
batch_size_ramp: 0
gradient_clipping: 0.5
pretrain_in_train_mode: False # default BERT trains with dropout layers enabled in pretrain

objective:
  name: masked-lm
  mlm_probability: 0.15
  use_80_20_rule: True
  disable_mlm: False
  token_drop: 0.0
reverse_dataset_order: False

budget: ${budget}

stacking:
  enabled: false
  num_initial_layers: 4
  num_layers_to_add: 12
  scheduler: manual
  adjust_lr: false
  freeze_bottom_layers: false
  manual_scheduler:
    function: manual
    balance_factor: 1.0
  T_max_factor: 0.75
  copy_optim_states: false
  step_fractions: [0.125, 0.3]
  doubling: true
  doubling_interpolation: false
  reset_optim: true

track_forward_pass_only: true

rho_loss:
  mega_batch_size: 15360
  il_losses_path: /home/jean/stackbert/outputs/examples_to_loss

sb:
  scale: 1.0

sophia:
  batch_size_hess_update: 768
  hess_update_frequency: 10
  free_updates: False

gradinit:
  enabled: False
  # eta: 1.0
  # tau: 1e-3 # step size
  # steps: 50
  # min_scale: 1e-3
  # max_scale: 1e3
  # step_type: sign-grad # sign-grad or grad
  # second_order: False
# sequence_curriculum:
#   lengths: [8,16,32,64,128]
#   triggers: [0.1,0.2,0.3,0.5,0.75]
#   unfold: False

# weight_averaging:
#   type: EMA
#   frequency: 1
#   momentum: 0.995 # only for EMA
#   last_k: 10

# CU1: +train.sequence_curriculum.lengths=[8,16,32,64,128] +train.sequence_curriculum.triggers=[0.1,0.2,0.3,0.5,0.75] +train.sequence_curriculum.unfold=False
# CU2: +train.sequence_curriculum.lengths=[8,16,32,64,128] +train.sequence_curriculum.triggers=[0.1,0.2,0.3,0.5,0.75] +train.sequence_curriculum.unfold=True

# LAWA: +train.weight_averaging.frequency=5000 +train.weight_averaging.type=LAWA +train.weight_averaging.last_k=10
# EMA: +train.weight_averaging.frequency=1 +train.weight_averaging.type=EMA +train.weight_averaging.momentum=0.995
