# These are the huggingface bert parameters
architectures:
  - BertForMaskedLM

attention_probs_dropout_prob: 0.1
hidden_act: gelu
hidden_dropout_prob: 0.1
hidden_size: 128
initializer_range: 0.02
intermediate_size: 512
layer_norm_eps: 1e-12
max_position_embeddings: 512
num_attention_heads: 2
num_hidden_layers: 2
pad_token_id: 0
position_embedding_type: absolute

type_vocab_size: 2
use_cache: true
# original bert-tiny hparams from https://github.com/google-research/bert:
# {"hidden_size": 128,
#  "hidden_act": "gelu",
#  "initializer_range": 0.02,
#  "vocab_size": 30522,
#  "hidden_dropout_prob": 0.1,
#  "num_attention_heads": 2,
#  "type_vocab_size": 2,
#  "max_position_embeddings": 512,
#  "num_hidden_layers": 2,
#  "intermediate_size": 512,
#  "attention_probs_dropout_prob": 0.1}
